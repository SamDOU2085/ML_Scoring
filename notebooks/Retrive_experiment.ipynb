{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0541a90e-fde0-4e32-9fc1-e46eb064728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_mlflow_model_perfs():\n",
    "    import mlflow\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # Connexion au serveur MLFlow\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    client = MlflowClient(\"http://127.0.0.1:5000\")\n",
    "    \n",
    "    try:\n",
    "        experiments = client.search_experiments(view_type=mlflow.entities.ViewType.ALL)\n",
    "        #print(\"Connexion au serveur MLflow. Expériences détectées:\")\n",
    "        for exp in experiments:\n",
    "            print(f\"Expérience ID: {exp.experiment_id}, Nom: {exp.name}\")\n",
    "    except Exception as e:\n",
    "        print(\"Échec de la connexion au serveur MLflow:\", e)\n",
    "    \n",
    "    # Utiliser l'experiment par défaut avec ID = 0\n",
    "    experiment_id = \"0\"\n",
    "    \n",
    "    # Récupérer les runs pour l'experiment par défaut\n",
    "    runs = client.search_runs(experiment_ids=[experiment_id])\n",
    "    \n",
    "    # Afficher les noms des runs\n",
    "    run_names = [run.data.tags.get(\"mlflow.runName\") for run in runs]\n",
    "    for idx, run_name in enumerate(run_names):\n",
    "        #print(f\"Run {idx+1}: {run_name}\")\n",
    "        None\n",
    "    \n",
    "    # Pour chaque run, lire les métriques loggées avec arrondi à 2 chiffres\n",
    "    Runs=[]\n",
    "    MetricName=[]\n",
    "    MetricValue=[]\n",
    "    for run in runs:\n",
    "        #print(f\"Run ID: {run.info.run_id}\")\n",
    "        #print(f\"Run Name: {run.data.tags.get('mlflow.runName')}\")\n",
    "        \n",
    "        # Afficher toutes les métriques loggées pour ce run\n",
    "        metrics = run.data.metrics\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            #print(f\"{metric_name}: {round(metric_value, 2)}\")\n",
    "            Runs.append(run.data.tags.get('mlflow.runName'))\n",
    "            MetricName.append(metric_name)\n",
    "            MetricValue.append(metric_value)\n",
    "        \n",
    "        #print(\"-\" * 40)\n",
    "    \n",
    "    \n",
    "    \n",
    "    compilation = pd.DataFrame({\"Runs\":Runs,\"MetricName\":MetricName,\"MetricValue\":MetricValue})\n",
    "    best_model_per_metric= compilation.groupby(\"MetricName\").apply(lambda x: x[[\"Runs\",\"MetricValue\"]][x[\"MetricValue\"]==np.max(x[\"MetricValue\"])] )\n",
    "    best_model_per_metric[\"MetricName\"]=[i[0] for i in best_model_per_metric.index]\n",
    "    #best_model_per_metric_test=best_model_per_metric.loc[best_model_per_metric.MetricName.str.contains(\"test\"),:]\n",
    "    #best_model_per_metric_test.sort_values(\"Runs\")\n",
    "    mlflow.end_run()\n",
    "    return best_model_per_metric #best_model_per_metric_test\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def collect_mlflow_model_hyperparams():\n",
    "    import mlflow\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    import pandas as pd\n",
    "\n",
    "    # Step 1: Connect to MLflow server\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    client = MlflowClient(\"http://127.0.0.1:5000\")\n",
    "\n",
    "    try:\n",
    "        # Step 2: Search for all experiments\n",
    "        experiments = client.search_experiments(view_type=mlflow.entities.ViewType.ALL)\n",
    "        print(\"Connexion au serveur MLflow. Expériences détectées:\")\n",
    "        for exp in experiments:\n",
    "            print(f\"Expérience ID: {exp.experiment_id}, Nom: {exp.name}\")\n",
    "    except Exception as e:\n",
    "        print(\"Échec de la connexion au serveur MLflow:\", e)\n",
    "        return\n",
    "\n",
    "    # Step 3: Use the default experiment (ID = 0)\n",
    "    experiment_id = \"0\"\n",
    "\n",
    "    # Step 4: Search for runs in the selected experiment\n",
    "    runs = client.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "    # Step 5: Initialize lists to store the hyperparameters (params) for each run\n",
    "    Runs = []\n",
    "    ParamName = []\n",
    "    ParamValue = []\n",
    "\n",
    "    # Step 6: Loop through each run and collect hyperparameters\n",
    "    for run in runs:\n",
    "        print(f\"Run ID: {run.info.run_id}\")\n",
    "        print(f\"Run Name: {run.data.tags.get('mlflow.runName')}\")\n",
    "\n",
    "        # Retrieve all logged hyperparameters (params) for the current run\n",
    "        params = run.data.params\n",
    "        for param_name, param_value in params.items():\n",
    "            print(f\"Hyperparameter - {param_name}: {param_value}\")\n",
    "            Runs.append(run.data.tags.get('mlflow.runName'))\n",
    "            ParamName.append(param_name)\n",
    "            ParamValue.append(param_value)\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    # Step 7: Create a DataFrame with the collected hyperparameters\n",
    "    hyperparams_df = pd.DataFrame({\"Run\": Runs, \"ParamName\": ParamName, \"ParamValue\": ParamValue})\n",
    "    \n",
    "    # Optional: Sort or filter based on hyperparameters, if needed\n",
    "    sorted_hyperparams_df = hyperparams_df.sort_values(by=[\"Run\", \"ParamName\"])\n",
    "\n",
    "    print(\"Hyperparameters collected:\")\n",
    "    #print(sorted_hyperparams_df)\n",
    "    mlflow.end_run()\n",
    "\n",
    "    return sorted_hyperparams_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a508c98-fc78-431b-868f-88739e69ac76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expérience ID: 0, Nom: Default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_224582/1909827535.py:53: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  best_model_per_metric= compilation.groupby(\"MetricName\").apply(lambda x: x[[\"Runs\",\"MetricValue\"]][x[\"MetricValue\"]==np.max(x[\"MetricValue\"])] )\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Runs</th>\n",
       "      <th>MetricValue</th>\n",
       "      <th>MetricName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression_F10</td>\n",
       "      <td>0.629123</td>\n",
       "      <td>cv_test_mean_score_fbeta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression_F10</td>\n",
       "      <td>0.634312</td>\n",
       "      <td>cv_train_mean_score_fbeta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression_F10</td>\n",
       "      <td>0.650386</td>\n",
       "      <td>test_fbeta_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression_F10</td>\n",
       "      <td>0.634181</td>\n",
       "      <td>train_fbeta_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression_ROC_AUC</td>\n",
       "      <td>0.734182</td>\n",
       "      <td>test_cv_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression_ROC_AUC</td>\n",
       "      <td>0.687327</td>\n",
       "      <td>test_roc_auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression_ROC_AUC</td>\n",
       "      <td>0.743167</td>\n",
       "      <td>train_cv_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression_ROC_AUC</td>\n",
       "      <td>0.742221</td>\n",
       "      <td>train_roc_auc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Runs  MetricValue                 MetricName\n",
       "0      LogisticRegression_F10     0.629123   cv_test_mean_score_fbeta\n",
       "1      LogisticRegression_F10     0.634312  cv_train_mean_score_fbeta\n",
       "3      LogisticRegression_F10     0.650386           test_fbeta_score\n",
       "6      LogisticRegression_F10     0.634181          train_fbeta_score\n",
       "2  LogisticRegression_ROC_AUC     0.734182               test_cv_mean\n",
       "4  LogisticRegression_ROC_AUC     0.687327               test_roc_auc\n",
       "5  LogisticRegression_ROC_AUC     0.743167              train_cv_mean\n",
       "7  LogisticRegression_ROC_AUC     0.742221              train_roc_auc"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=collect_mlflow_model_perfs()\n",
    "df=df.reset_index(drop=True)\n",
    "df.sort_values(['Runs','MetricName'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
